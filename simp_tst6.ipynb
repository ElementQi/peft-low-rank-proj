{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ct_model import DeltaModel, dispatch_default\n",
    "from ct_bnb import Linear4bit, dispatch_bnb_4bit\n",
    "from ct_layer import DeltaLayer\n",
    "from ct_config import CTConfig\n",
    "from ct_optim import BlockOptimizer\n",
    "from prepare_data import gen_dataloader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device_id = 2\n",
    "\n",
    "# Quantization type (fp4 or nf4), According to QLoRA paper, for training 4-bit base models (e.g. using LoRA adapters) one should use\n",
    "bnb_4bit_quant_type = \"fp4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "model_id = \"Qwen/Qwen1.5-0.5B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":device_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CTConfig(\n",
    "             r=32,\n",
    "             delta_alpha=32,\n",
    "             target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                             \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "             delta_dropout=0,\n",
    "             init_lora_weights=False,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = CTConfig(\n",
    "#              r=32,\n",
    "#              delta_alpha=32,\n",
    "#              target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#              delta_dropout=0,\n",
    "#              init_lora_weights=False,\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_model = DeltaModel(model, config, \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_optimizer = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.0.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.0.']\n"
     ]
    }
   ],
   "source": [
    "optimizer = BlockOptimizer(base_optimizer,\n",
    "                           list(delta_model.named_parameters()),\n",
    "                           None,\n",
    "                           10,\n",
    "                           switch_mode=\"ascending\",\n",
    "                           model=delta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/ubuntu/date/hf_cache/modules/datasets_modules/datasets/eli5_category/80106cc49322f1f5075e1387be4a5b74b95e0f56c40ff142b8999d0606aa1908 (last modified on Wed Jun  5 22:09:48 2024) since it couldn't be found locally at eli5_category, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35371bfbb1449d29599e9790ddaeb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38359175eff54fb1ae786a49e874c7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af43eb298c7a4425beea18779b139a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03be24adbae4b04be2c1656adab0a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader = gen_dataloader(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/800 [00:00<12:30,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1361, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/800 [00:01<03:08,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5185, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/800 [00:02<02:50,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.1.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.1.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 11/800 [00:03<04:39,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6131, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/800 [00:04<02:55,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3395, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/800 [00:05<02:46,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.2.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.2.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 21/800 [00:06<04:23,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1438, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26/800 [00:07<02:53,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8793, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 28/800 [00:08<02:45,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.3.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.3.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 31/800 [00:09<04:15,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8666, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 36/800 [00:10<02:47,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0670, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 38/800 [00:11<02:38,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.4.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.4.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 41/800 [00:12<04:10,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7212, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 46/800 [00:13<02:46,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1999, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 48/800 [00:13<02:37,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.5.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.5.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 51/800 [00:15<04:08,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0427, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 56/800 [00:16<02:42,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4293, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 58/800 [00:16<02:34,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.6.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.6.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 61/800 [00:18<04:03,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6852, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 66/800 [00:19<02:41,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3130, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 68/800 [00:19<02:30,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.7.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.7.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 71/800 [00:21<03:59,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4889, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 76/800 [00:22<02:38,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0927, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 78/800 [00:22<02:28,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.8.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.8.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 81/800 [00:23<03:52,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7633, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 86/800 [00:24<02:33,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4722, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 88/800 [00:25<02:25,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.9.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.9.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 91/800 [00:26<03:46,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9079, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 96/800 [00:27<02:31,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7587, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 98/800 [00:28<02:24,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now init the block adapter ['model.model.layers.10.']\n",
      "Parameters with the following prefix will be trainable: ['model.model.layers.10.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 101/800 [00:29<03:45,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2594, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 106/800 [00:30<02:28,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1296, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 107/800 [00:30<03:19,  3.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdelta_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/peft-single-adapter/src/peft/tuners/tuners_utils.py:180\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/accelerate/hooks.py:167\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/accelerate/hooks.py:380\u001b[0m, in \u001b[0;36mAlignDevicesHook.post_forward\u001b[0;34m(self, module, output)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_same_device \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/accelerate/utils/operations.py:186\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 186\u001b[0m         {\n\u001b[1;32m    187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/accelerate/utils/operations.py:187\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    186\u001b[0m         {\n\u001b[0;32m--> 187\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    189\u001b[0m         }\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m/home/ubuntu/date/mq_tst/anaconda3/envs/newpeft/lib/python3.10/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    outputs = delta_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    if step % 5 ==0:\n",
    "        print(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaModel(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-9): 10 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=32, bias=True)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=32, bias=True)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=32, bias=True)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=2816, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=2816, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2816, out_features=1024, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict(\n",
       "                (default): Linear(in_features=2816, out_features=32, bias=False)\n",
       "              )\n",
       "              (delta_B): ModuleDict(\n",
       "                (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "              )\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (10): Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2816, out_features=1024, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict(\n",
       "                (default): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "              )\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "        (11-23): 13 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2816, out_features=1024, bias=False)\n",
       "              (delta_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (delta_theta): ModuleDict()\n",
       "              (delta_A): ModuleDict()\n",
       "              (delta_B): ModuleDict()\n",
       "              (delta_embedding): ParameterDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpeft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
